{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81a808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 001, Train: 0.4563, Val: 0.4540, Test: 0.4050, Avg100: 32.4120\n",
      "Epoch: 002, Train: 0.3500, Val: 0.3500, Test: 0.3200, Avg100: 21.7147\n",
      "Epoch: 003, Train: 0.2875, Val: 0.2280, Test: 0.2100, Avg100: 16.3385\n",
      "Epoch: 004, Train: 0.2875, Val: 0.2260, Test: 0.2100, Avg100: 13.1128\n",
      "Epoch: 005, Train: 0.2688, Val: 0.2220, Test: 0.2030, Avg100: 10.9612\n",
      "Epoch: 006, Train: 0.2688, Val: 0.2220, Test: 0.2030, Avg100: 9.4243\n",
      "Epoch: 007, Train: 0.2688, Val: 0.2220, Test: 0.2020, Avg100: 8.2715\n",
      "Epoch: 008, Train: 0.2688, Val: 0.2220, Test: 0.2020, Avg100: 7.3749\n",
      "Epoch: 009, Train: 0.2688, Val: 0.2220, Test: 0.2020, Avg100: 6.6576\n",
      "Epoch: 010, Train: 0.2688, Val: 0.2220, Test: 0.2020, Avg100: 6.0707\n",
      "Epoch: 011, Train: 0.2688, Val: 0.2220, Test: 0.2020, Avg100: 5.5817\n",
      "Epoch: 012, Train: 0.2688, Val: 0.2220, Test: 0.2020, Avg100: 5.1678\n",
      "Epoch: 013, Train: 0.2688, Val: 0.2220, Test: 0.2020, Avg100: 4.8131\n",
      "Epoch: 014, Train: 0.2688, Val: 0.2220, Test: 0.2020, Avg100: 4.5057\n",
      "Epoch: 015, Train: 0.2688, Val: 0.2220, Test: 0.2020, Avg100: 4.2368\n",
      "Epoch: 016, Train: 0.2812, Val: 0.2240, Test: 0.2070, Avg100: 3.9997\n",
      "Epoch: 017, Train: 0.2875, Val: 0.2280, Test: 0.2100, Avg100: 3.7892\n",
      "Epoch: 018, Train: 0.3000, Val: 0.2640, Test: 0.2350, Avg100: 3.6021\n",
      "Epoch: 019, Train: 0.4125, Val: 0.4140, Test: 0.3840, Avg100: 3.4412\n",
      "Epoch: 020, Train: 0.4500, Val: 0.4460, Test: 0.4040, Avg100: 3.2966\n",
      "Epoch: 021, Train: 0.4563, Val: 0.4580, Test: 0.4100, Avg100: 3.1654\n",
      "Epoch: 022, Train: 0.4563, Val: 0.4600, Test: 0.4140, Avg100: 3.0457\n",
      "Epoch: 023, Train: 0.4625, Val: 0.4600, Test: 0.4150, Avg100: 2.9361\n",
      "Epoch: 024, Train: 0.4563, Val: 0.4600, Test: 0.4150, Avg100: 2.8353\n",
      "Epoch: 025, Train: 0.4625, Val: 0.4600, Test: 0.4110, Avg100: 2.7420\n",
      "Epoch: 026, Train: 0.4563, Val: 0.4560, Test: 0.4090, Avg100: 2.6556\n",
      "Epoch: 027, Train: 0.4500, Val: 0.4460, Test: 0.4050, Avg100: 2.5753\n",
      "Epoch: 028, Train: 0.4437, Val: 0.4300, Test: 0.3960, Avg100: 2.5001\n",
      "Epoch: 029, Train: 0.4250, Val: 0.4100, Test: 0.3830, Avg100: 2.4295\n",
      "Epoch: 030, Train: 0.4062, Val: 0.3740, Test: 0.3650, Avg100: 2.3629\n",
      "Epoch: 031, Train: 0.3750, Val: 0.3580, Test: 0.3340, Avg100: 2.2995\n",
      "Epoch: 032, Train: 0.3750, Val: 0.3480, Test: 0.3280, Avg100: 2.2398\n",
      "Epoch: 033, Train: 0.3938, Val: 0.3680, Test: 0.3590, Avg100: 2.1845\n",
      "Epoch: 034, Train: 0.4062, Val: 0.3800, Test: 0.3650, Avg100: 2.1325\n",
      "Epoch: 035, Train: 0.4125, Val: 0.4040, Test: 0.3750, Avg100: 2.0837\n",
      "Epoch: 036, Train: 0.4500, Val: 0.4320, Test: 0.3980, Avg100: 2.0381\n",
      "Epoch: 037, Train: 0.4500, Val: 0.4560, Test: 0.4080, Avg100: 1.9952\n",
      "Epoch: 038, Train: 0.4500, Val: 0.4600, Test: 0.4110, Avg100: 1.9546\n",
      "Epoch: 039, Train: 0.4563, Val: 0.4600, Test: 0.4150, Avg100: 1.9161\n",
      "Epoch: 040, Train: 0.4625, Val: 0.4620, Test: 0.4150, Avg100: 1.8795\n",
      "Epoch: 041, Train: 0.4563, Val: 0.4600, Test: 0.4150, Avg100: 1.8446\n",
      "Epoch: 042, Train: 0.4500, Val: 0.4600, Test: 0.4110, Avg100: 1.8113\n",
      "Epoch: 043, Train: 0.4500, Val: 0.4580, Test: 0.4090, Avg100: 1.7794\n",
      "Epoch: 044, Train: 0.4500, Val: 0.4560, Test: 0.4080, Avg100: 1.7489\n",
      "Epoch: 045, Train: 0.4500, Val: 0.4560, Test: 0.4080, Avg100: 1.7198\n",
      "Epoch: 046, Train: 0.4500, Val: 0.4600, Test: 0.4090, Avg100: 1.6919\n",
      "Epoch: 047, Train: 0.4563, Val: 0.4600, Test: 0.4140, Avg100: 1.6653\n",
      "Epoch: 048, Train: 0.4563, Val: 0.4600, Test: 0.4150, Avg100: 1.6398\n",
      "Epoch: 049, Train: 0.4563, Val: 0.4600, Test: 0.4140, Avg100: 1.6152\n",
      "Epoch: 050, Train: 0.4563, Val: 0.4620, Test: 0.4120, Avg100: 0.3337\n",
      "Epoch: 051, Train: 0.4500, Val: 0.4620, Test: 0.4090, Avg100: 0.3381\n",
      "Epoch: 052, Train: 0.4563, Val: 0.4580, Test: 0.4100, Avg100: 0.3452\n",
      "Epoch: 053, Train: 0.5125, Val: 0.5000, Test: 0.4310, Avg100: 0.3533\n",
      "Epoch: 054, Train: 0.5562, Val: 0.5240, Test: 0.4700, Avg100: 0.3626\n",
      "Epoch: 055, Train: 0.5688, Val: 0.5420, Test: 0.4790, Avg100: 0.3726\n",
      "Epoch: 056, Train: 0.5562, Val: 0.5360, Test: 0.4810, Avg100: 0.3830\n",
      "Epoch: 057, Train: 0.5312, Val: 0.5040, Test: 0.4420, Avg100: 0.3930\n",
      "Epoch: 058, Train: 0.5063, Val: 0.4860, Test: 0.4310, Avg100: 0.4030\n",
      "Epoch: 059, Train: 0.4688, Val: 0.4640, Test: 0.4160, Avg100: 0.4080\n",
      "Epoch: 060, Train: 0.4625, Val: 0.4620, Test: 0.4150, Avg100: 0.4082\n",
      "Epoch: 061, Train: 0.4688, Val: 0.4640, Test: 0.4170, Avg100: 0.4081\n",
      "Epoch: 062, Train: 0.4625, Val: 0.4640, Test: 0.4200, Avg100: 0.4082\n",
      "Epoch: 063, Train: 0.4625, Val: 0.4680, Test: 0.4180, Avg100: 0.4085\n",
      "Epoch: 064, Train: 0.4625, Val: 0.4660, Test: 0.4180, Avg100: 0.4099\n",
      "Epoch: 065, Train: 0.4688, Val: 0.4680, Test: 0.4190, Avg100: 0.4137\n",
      "Epoch: 066, Train: 0.4688, Val: 0.4660, Test: 0.4190, Avg100: 0.4181\n",
      "Epoch: 067, Train: 0.4688, Val: 0.4700, Test: 0.4210, Avg100: 0.4212\n",
      "Epoch: 068, Train: 0.4938, Val: 0.4940, Test: 0.4380, Avg100: 0.4229\n",
      "Epoch: 069, Train: 0.5875, Val: 0.5480, Test: 0.4800, Avg100: 0.4255\n",
      "Epoch: 070, Train: 0.6750, Val: 0.6080, Test: 0.5490, Avg100: 0.4304\n",
      "Epoch: 071, Train: 0.7000, Val: 0.6120, Test: 0.5670, Avg100: 0.4368\n",
      "Epoch: 072, Train: 0.6750, Val: 0.6160, Test: 0.5580, Avg100: 0.4434\n",
      "Epoch: 073, Train: 0.6687, Val: 0.6020, Test: 0.5510, Avg100: 0.4500\n",
      "Epoch: 074, Train: 0.6750, Val: 0.5940, Test: 0.5500, Avg100: 0.4568\n",
      "Epoch: 075, Train: 0.6562, Val: 0.5920, Test: 0.5420, Avg100: 0.4643\n",
      "Epoch: 076, Train: 0.6313, Val: 0.5760, Test: 0.5170, Avg100: 0.4703\n",
      "Epoch: 077, Train: 0.5875, Val: 0.5560, Test: 0.4960, Avg100: 0.4711\n",
      "Epoch: 078, Train: 0.5625, Val: 0.5260, Test: 0.4710, Avg100: 0.4720\n",
      "Epoch: 079, Train: 0.5375, Val: 0.5200, Test: 0.4650, Avg100: 0.4766\n",
      "Epoch: 080, Train: 0.5562, Val: 0.5260, Test: 0.4780, Avg100: 0.4830\n",
      "Epoch: 081, Train: 0.5562, Val: 0.5240, Test: 0.4830, Avg100: 0.4901\n",
      "Epoch: 082, Train: 0.5813, Val: 0.5320, Test: 0.4860, Avg100: 0.4983\n",
      "Epoch: 083, Train: 0.6250, Val: 0.5680, Test: 0.5120, Avg100: 0.5089\n",
      "Epoch: 084, Train: 0.6875, Val: 0.6080, Test: 0.5590, Avg100: 0.5189\n",
      "Epoch: 085, Train: 0.7188, Val: 0.6280, Test: 0.5890, Avg100: 0.5184\n",
      "Epoch: 086, Train: 0.7250, Val: 0.6340, Test: 0.5890, Avg100: 0.5182\n",
      "Epoch: 087, Train: 0.7250, Val: 0.6440, Test: 0.5960, Avg100: 0.5201\n",
      "Epoch: 088, Train: 0.7312, Val: 0.6440, Test: 0.5970, Avg100: 0.5295\n",
      "Epoch: 089, Train: 0.7125, Val: 0.6440, Test: 0.5970, Avg100: 0.5486\n",
      "Epoch: 090, Train: 0.6875, Val: 0.6300, Test: 0.5720, Avg100: 0.5663\n",
      "Epoch: 091, Train: 0.6687, Val: 0.6020, Test: 0.5550, Avg100: 0.5818\n",
      "Epoch: 092, Train: 0.6438, Val: 0.5900, Test: 0.5350, Avg100: 0.5773\n",
      "Epoch: 093, Train: 0.6313, Val: 0.5760, Test: 0.5240, Avg100: 0.5633\n",
      "Epoch: 094, Train: 0.6375, Val: 0.5820, Test: 0.5290, Avg100: 0.5430\n",
      "Epoch: 095, Train: 0.6375, Val: 0.5800, Test: 0.5300, Avg100: 0.5295\n",
      "Epoch: 096, Train: 0.6625, Val: 0.5960, Test: 0.5490, Avg100: 0.5360\n",
      "Epoch: 097, Train: 0.7000, Val: 0.6300, Test: 0.5770, Avg100: 0.5630\n",
      "Epoch: 098, Train: 0.7188, Val: 0.6480, Test: 0.6040, Avg100: 0.6040\n",
      "Epoch: 099, Train: 0.7375, Val: 0.6680, Test: 0.6220, Avg100: 1.0576\n",
      "Epoch: 100, Train: 0.7500, Val: 0.6700, Test: 0.6210, Avg100: 0.4196\n",
      "Epoch: 101, Train: 0.7375, Val: 0.6540, Test: 0.6050, Avg100: 0.4216\n",
      "Epoch: 102, Train: 0.7250, Val: 0.6300, Test: 0.5900, Avg100: 0.4243\n",
      "Epoch: 103, Train: 0.7000, Val: 0.6140, Test: 0.5730, Avg100: 0.4279\n",
      "Epoch: 104, Train: 0.6875, Val: 0.6120, Test: 0.5690, Avg100: 0.4315\n",
      "Epoch: 105, Train: 0.7000, Val: 0.6200, Test: 0.5710, Avg100: 0.4352\n",
      "Epoch: 106, Train: 0.7125, Val: 0.6460, Test: 0.5950, Avg100: 0.4391\n",
      "Epoch: 107, Train: 0.7250, Val: 0.6640, Test: 0.6050, Avg100: 0.4431\n",
      "Epoch: 108, Train: 0.7375, Val: 0.6780, Test: 0.6120, Avg100: 0.4472\n",
      "Epoch: 109, Train: 0.7500, Val: 0.6760, Test: 0.6220, Avg100: 0.4514\n",
      "Epoch: 110, Train: 0.7625, Val: 0.6640, Test: 0.6170, Avg100: 0.4556\n",
      "Epoch: 111, Train: 0.7125, Val: 0.6400, Test: 0.5900, Avg100: 0.4595\n",
      "Epoch: 112, Train: 0.7000, Val: 0.6220, Test: 0.5800, Avg100: 0.4633\n",
      "Epoch: 113, Train: 0.7125, Val: 0.6340, Test: 0.5870, Avg100: 0.4671\n",
      "Epoch: 114, Train: 0.7437, Val: 0.6440, Test: 0.6030, Avg100: 0.4711\n",
      "Epoch: 115, Train: 0.7437, Val: 0.6720, Test: 0.6200, Avg100: 0.4753\n",
      "Epoch: 116, Train: 0.7375, Val: 0.6720, Test: 0.6120, Avg100: 0.4793\n",
      "Epoch: 117, Train: 0.7375, Val: 0.6700, Test: 0.6110, Avg100: 0.4834\n",
      "Epoch: 118, Train: 0.7437, Val: 0.6680, Test: 0.6200, Avg100: 0.4872\n",
      "Epoch: 119, Train: 0.7437, Val: 0.6500, Test: 0.6050, Avg100: 0.4894\n",
      "Epoch: 120, Train: 0.7250, Val: 0.6420, Test: 0.5940, Avg100: 0.4913\n",
      "Epoch: 121, Train: 0.7312, Val: 0.6400, Test: 0.5900, Avg100: 0.4931\n",
      "Epoch: 122, Train: 0.7500, Val: 0.6520, Test: 0.6050, Avg100: 0.4950\n",
      "Epoch: 123, Train: 0.7688, Val: 0.6780, Test: 0.6220, Avg100: 0.4971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Train: 0.7500, Val: 0.6840, Test: 0.6230, Avg100: 0.4992\n",
      "Epoch: 125, Train: 0.7500, Val: 0.6800, Test: 0.6260, Avg100: 0.5013\n",
      "Epoch: 126, Train: 0.7563, Val: 0.6780, Test: 0.6250, Avg100: 0.5035\n",
      "Epoch: 127, Train: 0.7563, Val: 0.6700, Test: 0.6240, Avg100: 0.5057\n",
      "Epoch: 128, Train: 0.7375, Val: 0.6480, Test: 0.6110, Avg100: 0.5078\n",
      "Epoch: 129, Train: 0.7437, Val: 0.6440, Test: 0.6010, Avg100: 0.5100\n",
      "Epoch: 130, Train: 0.7437, Val: 0.6440, Test: 0.6020, Avg100: 0.5124\n",
      "Epoch: 131, Train: 0.7437, Val: 0.6460, Test: 0.6030, Avg100: 0.5151\n",
      "Epoch: 132, Train: 0.7563, Val: 0.6540, Test: 0.6180, Avg100: 0.5180\n",
      "Epoch: 133, Train: 0.7688, Val: 0.6740, Test: 0.6250, Avg100: 0.5206\n",
      "Epoch: 134, Train: 0.7625, Val: 0.6760, Test: 0.6290, Avg100: 0.5233\n",
      "Epoch: 135, Train: 0.7625, Val: 0.6760, Test: 0.6300, Avg100: 0.5258\n",
      "Epoch: 136, Train: 0.7688, Val: 0.6780, Test: 0.6290, Avg100: 0.5281\n",
      "Epoch: 137, Train: 0.7688, Val: 0.6800, Test: 0.6330, Avg100: 0.5304\n",
      "Epoch: 138, Train: 0.7688, Val: 0.6800, Test: 0.6280, Avg100: 0.5325\n",
      "Epoch: 139, Train: 0.7688, Val: 0.6720, Test: 0.6270, Avg100: 0.5347\n",
      "Epoch: 140, Train: 0.7437, Val: 0.6460, Test: 0.6030, Avg100: 0.5365\n",
      "Epoch: 141, Train: 0.7312, Val: 0.6420, Test: 0.5950, Avg100: 0.5383\n",
      "Epoch: 142, Train: 0.7375, Val: 0.6440, Test: 0.6050, Avg100: 0.5403\n",
      "Epoch: 143, Train: 0.7688, Val: 0.6740, Test: 0.6230, Avg100: 0.5424\n",
      "Epoch: 144, Train: 0.7500, Val: 0.6760, Test: 0.6230, Avg100: 0.5446\n",
      "Epoch: 145, Train: 0.7500, Val: 0.6740, Test: 0.6240, Avg100: 0.5467\n",
      "Epoch: 146, Train: 0.7625, Val: 0.6820, Test: 0.6280, Avg100: 0.5489\n",
      "Epoch: 147, Train: 0.7688, Val: 0.6840, Test: 0.6300, Avg100: 0.5511\n",
      "Epoch: 148, Train: 0.7563, Val: 0.6860, Test: 0.6270, Avg100: 0.5532\n",
      "Epoch: 149, Train: 0.7500, Val: 0.6720, Test: 0.6170, Avg100: 0.5552\n",
      "Epoch: 150, Train: 0.7250, Val: 0.6500, Test: 0.6020, Avg100: 0.5571\n",
      "Epoch: 151, Train: 0.7500, Val: 0.6520, Test: 0.6100, Avg100: 0.5591\n",
      "Epoch: 152, Train: 0.7688, Val: 0.6720, Test: 0.6220, Avg100: 0.5613\n",
      "Epoch: 153, Train: 0.7500, Val: 0.6760, Test: 0.6210, Avg100: 0.5632\n",
      "Epoch: 154, Train: 0.7375, Val: 0.6740, Test: 0.6200, Avg100: 0.5647\n",
      "Epoch: 155, Train: 0.7500, Val: 0.6740, Test: 0.6230, Avg100: 0.5661\n",
      "Epoch: 156, Train: 0.7688, Val: 0.6800, Test: 0.6290, Avg100: 0.5676\n",
      "Epoch: 157, Train: 0.7625, Val: 0.6800, Test: 0.6280, Avg100: 0.5694\n",
      "Epoch: 158, Train: 0.7500, Val: 0.6800, Test: 0.6240, Avg100: 0.5714\n",
      "Epoch: 159, Train: 0.7563, Val: 0.6820, Test: 0.6250, Avg100: 0.5735\n",
      "Epoch: 160, Train: 0.7688, Val: 0.6820, Test: 0.6290, Avg100: 0.5756\n",
      "Epoch: 161, Train: 0.7688, Val: 0.6800, Test: 0.6290, Avg100: 0.5777\n",
      "Epoch: 162, Train: 0.7500, Val: 0.6800, Test: 0.6250, Avg100: 0.5798\n",
      "Epoch: 163, Train: 0.7500, Val: 0.6800, Test: 0.6240, Avg100: 0.5818\n",
      "Epoch: 164, Train: 0.7500, Val: 0.6740, Test: 0.6240, Avg100: 0.5839\n",
      "Epoch: 165, Train: 0.7688, Val: 0.6720, Test: 0.6210, Avg100: 0.5859\n",
      "Epoch: 166, Train: 0.7625, Val: 0.6700, Test: 0.6230, Avg100: 0.5880\n",
      "Epoch: 167, Train: 0.7625, Val: 0.6720, Test: 0.6250, Avg100: 0.5900\n",
      "Epoch: 168, Train: 0.7688, Val: 0.6820, Test: 0.6290, Avg100: 0.5919\n",
      "Epoch: 169, Train: 0.7688, Val: 0.6820, Test: 0.6310, Avg100: 0.5934\n",
      "Epoch: 170, Train: 0.7688, Val: 0.6780, Test: 0.6320, Avg100: 0.5942\n",
      "Epoch: 171, Train: 0.7688, Val: 0.6820, Test: 0.6320, Avg100: 0.5949\n",
      "Epoch: 172, Train: 0.7688, Val: 0.6820, Test: 0.6300, Avg100: 0.5956\n",
      "Epoch: 173, Train: 0.7688, Val: 0.6820, Test: 0.6290, Avg100: 0.5964\n",
      "Epoch: 174, Train: 0.7625, Val: 0.6740, Test: 0.6240, Avg100: 0.5971\n",
      "Epoch: 175, Train: 0.7437, Val: 0.6560, Test: 0.6120, Avg100: 0.5978\n",
      "Epoch: 176, Train: 0.7688, Val: 0.6740, Test: 0.6240, Avg100: 0.5989\n",
      "Epoch: 177, Train: 0.7688, Val: 0.6800, Test: 0.6280, Avg100: 0.6002\n",
      "Epoch: 178, Train: 0.7688, Val: 0.6800, Test: 0.6290, Avg100: 0.6018\n",
      "Epoch: 179, Train: 0.7688, Val: 0.6800, Test: 0.6300, Avg100: 0.6035\n",
      "Epoch: 180, Train: 0.7688, Val: 0.6800, Test: 0.6300, Avg100: 0.6050\n",
      "Epoch: 181, Train: 0.7625, Val: 0.6820, Test: 0.6290, Avg100: 0.6064\n",
      "Epoch: 182, Train: 0.7625, Val: 0.6820, Test: 0.6280, Avg100: 0.6079\n",
      "Epoch: 183, Train: 0.7688, Val: 0.6800, Test: 0.6270, Avg100: 0.6090\n",
      "Epoch: 184, Train: 0.7688, Val: 0.6800, Test: 0.6270, Avg100: 0.6097\n",
      "Epoch: 185, Train: 0.7750, Val: 0.6800, Test: 0.6320, Avg100: 0.6101\n",
      "Epoch: 186, Train: 0.7750, Val: 0.6820, Test: 0.6310, Avg100: 0.6105\n",
      "Epoch: 187, Train: 0.7750, Val: 0.6780, Test: 0.6300, Avg100: 0.6109\n",
      "Epoch: 188, Train: 0.7750, Val: 0.6820, Test: 0.6300, Avg100: 0.6112\n",
      "Epoch: 189, Train: 0.7688, Val: 0.6820, Test: 0.6320, Avg100: 0.6116\n",
      "Epoch: 190, Train: 0.7563, Val: 0.6820, Test: 0.6290, Avg100: 0.6121\n",
      "Epoch: 191, Train: 0.7563, Val: 0.6800, Test: 0.6280, Avg100: 0.6129\n",
      "Epoch: 192, Train: 0.7625, Val: 0.6780, Test: 0.6290, Avg100: 0.6138\n",
      "Epoch: 193, Train: 0.7625, Val: 0.6780, Test: 0.6280, Avg100: 0.6148\n",
      "Epoch: 194, Train: 0.7625, Val: 0.6800, Test: 0.6290, Avg100: 0.6158\n",
      "Epoch: 195, Train: 0.7625, Val: 0.6760, Test: 0.6290, Avg100: 0.6168\n",
      "Epoch: 196, Train: 0.7500, Val: 0.6720, Test: 0.6230, Avg100: 0.6176\n",
      "Epoch: 197, Train: 0.7563, Val: 0.6760, Test: 0.6220, Avg100: 0.6180\n",
      "Epoch: 198, Train: 0.7563, Val: 0.6760, Test: 0.6230, Avg100: 0.6182\n",
      "Epoch: 199, Train: 0.7750, Val: 0.6820, Test: 0.6340, Avg100: 0.6183\n",
      "Epoch: 200, Train: 0.7750, Val: 0.6820, Test: 0.6340, Avg100: 0.6185\n",
      "Epoch: 201, Train: 0.7750, Val: 0.6820, Test: 0.6340, Avg100: 0.6187\n",
      "Epoch: 202, Train: 0.7625, Val: 0.6820, Test: 0.6270, Avg100: 0.6191\n",
      "Epoch: 203, Train: 0.7625, Val: 0.6780, Test: 0.6270, Avg100: 0.6197\n",
      "Epoch: 204, Train: 0.7625, Val: 0.6760, Test: 0.6270, Avg100: 0.6202\n",
      "Epoch: 205, Train: 0.7750, Val: 0.6800, Test: 0.6270, Avg100: 0.6208\n",
      "Epoch: 206, Train: 0.7563, Val: 0.6800, Test: 0.6250, Avg100: 0.6211\n",
      "Epoch: 207, Train: 0.7563, Val: 0.6800, Test: 0.6280, Avg100: 0.6213\n",
      "Epoch: 208, Train: 0.7750, Val: 0.6800, Test: 0.6330, Avg100: 0.6215\n",
      "Epoch: 209, Train: 0.7750, Val: 0.6780, Test: 0.6300, Avg100: 0.6216\n",
      "Epoch: 210, Train: 0.7750, Val: 0.6800, Test: 0.6300, Avg100: 0.6217\n",
      "Epoch: 211, Train: 0.7750, Val: 0.6800, Test: 0.6320, Avg100: 0.6222\n",
      "Epoch: 212, Train: 0.7750, Val: 0.6800, Test: 0.6300, Avg100: 0.6227\n",
      "Epoch: 213, Train: 0.7750, Val: 0.6800, Test: 0.6300, Avg100: 0.6231\n",
      "Epoch: 214, Train: 0.7750, Val: 0.6800, Test: 0.6310, Avg100: 0.6234\n",
      "Epoch: 215, Train: 0.7750, Val: 0.6800, Test: 0.6310, Avg100: 0.6235\n",
      "Epoch: 216, Train: 0.7812, Val: 0.6800, Test: 0.6340, Avg100: 0.6237\n",
      "Epoch: 217, Train: 0.7812, Val: 0.6840, Test: 0.6380, Avg100: 0.6240\n",
      "Epoch: 218, Train: 0.7812, Val: 0.6820, Test: 0.6370, Avg100: 0.6241\n",
      "Epoch: 219, Train: 0.7688, Val: 0.6740, Test: 0.6260, Avg100: 0.6244\n",
      "Epoch: 220, Train: 0.7563, Val: 0.6680, Test: 0.6160, Avg100: 0.6246\n",
      "Epoch: 221, Train: 0.7563, Val: 0.6620, Test: 0.6140, Avg100: 0.6248\n",
      "Epoch: 222, Train: 0.7625, Val: 0.6680, Test: 0.6210, Avg100: 0.6250\n",
      "Epoch: 223, Train: 0.7688, Val: 0.6760, Test: 0.6280, Avg100: 0.6250\n",
      "Epoch: 224, Train: 0.7688, Val: 0.6740, Test: 0.6290, Avg100: 0.6251\n",
      "Epoch: 225, Train: 0.7688, Val: 0.6780, Test: 0.6290, Avg100: 0.6251\n",
      "Epoch: 226, Train: 0.7812, Val: 0.6800, Test: 0.6300, Avg100: 0.6252\n",
      "Epoch: 227, Train: 0.7750, Val: 0.6800, Test: 0.6300, Avg100: 0.6252\n",
      "Epoch: 228, Train: 0.7812, Val: 0.6780, Test: 0.6280, Avg100: 0.6254\n",
      "Epoch: 229, Train: 0.7812, Val: 0.6800, Test: 0.6280, Avg100: 0.6257\n",
      "Epoch: 230, Train: 0.7625, Val: 0.6760, Test: 0.6250, Avg100: 0.6259\n",
      "Epoch: 231, Train: 0.7688, Val: 0.6780, Test: 0.6290, Avg100: 0.6262\n",
      "Epoch: 232, Train: 0.7812, Val: 0.6800, Test: 0.6330, Avg100: 0.6263\n",
      "Epoch: 233, Train: 0.7812, Val: 0.6800, Test: 0.6320, Avg100: 0.6264\n",
      "Epoch: 234, Train: 0.7688, Val: 0.6800, Test: 0.6290, Avg100: 0.6264\n",
      "Epoch: 235, Train: 0.7688, Val: 0.6800, Test: 0.6290, Avg100: 0.6264\n",
      "Epoch: 236, Train: 0.7812, Val: 0.6800, Test: 0.6320, Avg100: 0.6264\n",
      "Epoch: 237, Train: 0.7875, Val: 0.6780, Test: 0.6370, Avg100: 0.6264\n",
      "Epoch: 238, Train: 0.7750, Val: 0.6720, Test: 0.6320, Avg100: 0.6265\n",
      "Epoch: 239, Train: 0.7063, Val: 0.6280, Test: 0.5800, Avg100: 0.6260\n",
      "Epoch: 240, Train: 0.6562, Val: 0.5700, Test: 0.5300, Avg100: 0.6253\n",
      "Epoch: 241, Train: 0.6812, Val: 0.5840, Test: 0.5340, Avg100: 0.6247\n",
      "Epoch: 242, Train: 0.7188, Val: 0.6160, Test: 0.5740, Avg100: 0.6244\n",
      "Epoch: 243, Train: 0.7688, Val: 0.6540, Test: 0.6200, Avg100: 0.6243\n",
      "Epoch: 244, Train: 0.7937, Val: 0.7100, Test: 0.6470, Avg100: 0.6246\n",
      "Epoch: 245, Train: 0.7875, Val: 0.7140, Test: 0.6540, Avg100: 0.6249\n",
      "Epoch: 246, Train: 0.7875, Val: 0.7220, Test: 0.6680, Avg100: 0.6253\n",
      "Epoch: 247, Train: 0.8000, Val: 0.7180, Test: 0.6640, Avg100: 0.6256\n",
      "Epoch: 248, Train: 0.7812, Val: 0.6800, Test: 0.6310, Avg100: 0.6257\n",
      "Epoch: 249, Train: 0.7750, Val: 0.6800, Test: 0.6290, Avg100: 0.6258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250, Train: 0.7750, Val: 0.6720, Test: 0.6240, Avg100: 0.6260\n",
      "Epoch: 251, Train: 0.7750, Val: 0.6620, Test: 0.6200, Avg100: 0.6261\n",
      "Epoch: 252, Train: 0.7625, Val: 0.6580, Test: 0.6170, Avg100: 0.6260\n",
      "Epoch: 253, Train: 0.7750, Val: 0.6660, Test: 0.6270, Avg100: 0.6261\n",
      "Epoch: 254, Train: 0.7875, Val: 0.6740, Test: 0.6360, Avg100: 0.6263\n",
      "Epoch: 255, Train: 0.7875, Val: 0.6820, Test: 0.6320, Avg100: 0.6264\n",
      "Epoch: 256, Train: 0.7875, Val: 0.6900, Test: 0.6350, Avg100: 0.6264\n",
      "Epoch: 257, Train: 0.7750, Val: 0.6920, Test: 0.6360, Avg100: 0.6265\n",
      "Epoch: 258, Train: 0.7750, Val: 0.6840, Test: 0.6330, Avg100: 0.6266\n",
      "Epoch: 259, Train: 0.7875, Val: 0.6940, Test: 0.6520, Avg100: 0.6269\n",
      "Epoch: 260, Train: 0.8188, Val: 0.7400, Test: 0.6930, Avg100: 0.6275\n",
      "Epoch: 261, Train: 0.8188, Val: 0.7460, Test: 0.6970, Avg100: 0.6282\n",
      "Epoch: 262, Train: 0.8250, Val: 0.7520, Test: 0.7020, Avg100: 0.6289\n",
      "Epoch: 263, Train: 0.8125, Val: 0.7200, Test: 0.6680, Avg100: 0.6294\n",
      "Epoch: 264, Train: 0.7875, Val: 0.6960, Test: 0.6360, Avg100: 0.6295\n",
      "Epoch: 265, Train: 0.7812, Val: 0.6880, Test: 0.6330, Avg100: 0.6296\n",
      "Epoch: 266, Train: 0.7875, Val: 0.6860, Test: 0.6330, Avg100: 0.6297\n",
      "Epoch: 267, Train: 0.7937, Val: 0.6820, Test: 0.6380, Avg100: 0.6299\n",
      "Epoch: 268, Train: 0.7937, Val: 0.6840, Test: 0.6420, Avg100: 0.6300\n",
      "Epoch: 269, Train: 0.7875, Val: 0.6860, Test: 0.6440, Avg100: 0.6301\n",
      "Epoch: 270, Train: 0.7937, Val: 0.6880, Test: 0.6430, Avg100: 0.6302\n",
      "Epoch: 271, Train: 0.7937, Val: 0.6880, Test: 0.6420, Avg100: 0.6303\n",
      "Epoch: 272, Train: 0.7812, Val: 0.6940, Test: 0.6350, Avg100: 0.6304\n",
      "Epoch: 273, Train: 0.7750, Val: 0.6920, Test: 0.6310, Avg100: 0.6304\n",
      "Epoch: 274, Train: 0.7812, Val: 0.6940, Test: 0.6380, Avg100: 0.6305\n",
      "Epoch: 275, Train: 0.8000, Val: 0.7020, Test: 0.6550, Avg100: 0.6310\n",
      "Epoch: 276, Train: 0.8000, Val: 0.6980, Test: 0.6570, Avg100: 0.6313\n",
      "Epoch: 277, Train: 0.8062, Val: 0.6980, Test: 0.6530, Avg100: 0.6315\n",
      "Epoch: 278, Train: 0.7937, Val: 0.6840, Test: 0.6440, Avg100: 0.6317\n",
      "Epoch: 279, Train: 0.7937, Val: 0.6880, Test: 0.6390, Avg100: 0.6318\n",
      "Epoch: 280, Train: 0.7937, Val: 0.6820, Test: 0.6360, Avg100: 0.6318\n",
      "Epoch: 281, Train: 0.7937, Val: 0.6860, Test: 0.6360, Avg100: 0.6319\n",
      "Epoch: 282, Train: 0.7937, Val: 0.6860, Test: 0.6440, Avg100: 0.6321\n",
      "Epoch: 283, Train: 0.7937, Val: 0.6920, Test: 0.6480, Avg100: 0.6323\n",
      "Epoch: 284, Train: 0.8062, Val: 0.7060, Test: 0.6590, Avg100: 0.6326\n",
      "Epoch: 285, Train: 0.8062, Val: 0.7000, Test: 0.6570, Avg100: 0.6329\n",
      "Epoch: 286, Train: 0.7875, Val: 0.6920, Test: 0.6480, Avg100: 0.6330\n",
      "Epoch: 287, Train: 0.7875, Val: 0.6920, Test: 0.6380, Avg100: 0.6331\n",
      "Epoch: 288, Train: 0.7812, Val: 0.7000, Test: 0.6460, Avg100: 0.6333\n",
      "Epoch: 289, Train: 0.8125, Val: 0.7100, Test: 0.6640, Avg100: 0.6336\n",
      "Epoch: 290, Train: 0.8125, Val: 0.7120, Test: 0.6620, Avg100: 0.6339\n",
      "Epoch: 291, Train: 0.8000, Val: 0.7080, Test: 0.6560, Avg100: 0.6342\n",
      "Epoch: 292, Train: 0.7875, Val: 0.6940, Test: 0.6420, Avg100: 0.6343\n",
      "Epoch: 293, Train: 0.7875, Val: 0.6900, Test: 0.6380, Avg100: 0.6344\n",
      "Epoch: 294, Train: 0.7875, Val: 0.6880, Test: 0.6370, Avg100: 0.6345\n",
      "Epoch: 295, Train: 0.7937, Val: 0.6920, Test: 0.6460, Avg100: 0.6347\n",
      "Epoch: 296, Train: 0.8062, Val: 0.7040, Test: 0.6560, Avg100: 0.6350\n",
      "Epoch: 297, Train: 0.8062, Val: 0.7140, Test: 0.6650, Avg100: 0.6354\n",
      "Epoch: 298, Train: 0.8125, Val: 0.7080, Test: 0.6640, Avg100: 0.6358\n",
      "Epoch: 299, Train: 0.7875, Val: 0.6980, Test: 0.6490, Avg100: 0.6360\n",
      "Epoch: 300, Train: 0.7812, Val: 0.6940, Test: 0.6390, Avg100: 0.6360\n",
      "Epoch: 301, Train: 0.7812, Val: 0.6960, Test: 0.6430, Avg100: 0.6361\n",
      "Epoch: 302, Train: 0.8000, Val: 0.7100, Test: 0.6660, Avg100: 0.6365\n",
      "Epoch: 303, Train: 0.8250, Val: 0.7300, Test: 0.6860, Avg100: 0.6371\n",
      "Epoch: 304, Train: 0.8188, Val: 0.7180, Test: 0.6910, Avg100: 0.6378\n",
      "Epoch: 305, Train: 0.8062, Val: 0.7040, Test: 0.6590, Avg100: 0.6381\n",
      "Epoch: 306, Train: 0.8000, Val: 0.6960, Test: 0.6530, Avg100: 0.6384\n",
      "Epoch: 307, Train: 0.7937, Val: 0.6940, Test: 0.6470, Avg100: 0.6385\n",
      "Epoch: 308, Train: 0.8062, Val: 0.7000, Test: 0.6520, Avg100: 0.6387\n",
      "Epoch: 309, Train: 0.8062, Val: 0.7000, Test: 0.6480, Avg100: 0.6389\n",
      "Epoch: 310, Train: 0.8000, Val: 0.7020, Test: 0.6500, Avg100: 0.6391\n",
      "Epoch: 311, Train: 0.8000, Val: 0.7120, Test: 0.6550, Avg100: 0.6393\n",
      "Epoch: 312, Train: 0.8188, Val: 0.7200, Test: 0.6700, Avg100: 0.6397\n",
      "Epoch: 313, Train: 0.8250, Val: 0.7400, Test: 0.6940, Avg100: 0.6404\n",
      "Epoch: 314, Train: 0.8250, Val: 0.7380, Test: 0.6960, Avg100: 0.6410\n",
      "Epoch: 315, Train: 0.8188, Val: 0.7240, Test: 0.6830, Avg100: 0.6416\n",
      "Epoch: 316, Train: 0.8000, Val: 0.7140, Test: 0.6550, Avg100: 0.6418\n",
      "Epoch: 317, Train: 0.8000, Val: 0.7040, Test: 0.6480, Avg100: 0.6419\n",
      "Epoch: 318, Train: 0.8000, Val: 0.7080, Test: 0.6510, Avg100: 0.6420\n",
      "Epoch: 319, Train: 0.8062, Val: 0.7100, Test: 0.6580, Avg100: 0.6423\n",
      "Epoch: 320, Train: 0.8062, Val: 0.7120, Test: 0.6660, Avg100: 0.6428\n",
      "Epoch: 321, Train: 0.8125, Val: 0.7120, Test: 0.6710, Avg100: 0.6434\n",
      "Epoch: 322, Train: 0.8062, Val: 0.7100, Test: 0.6610, Avg100: 0.6438\n",
      "Epoch: 323, Train: 0.8062, Val: 0.7060, Test: 0.6510, Avg100: 0.6440\n",
      "Epoch: 324, Train: 0.8000, Val: 0.7060, Test: 0.6510, Avg100: 0.6442\n",
      "Epoch: 325, Train: 0.8062, Val: 0.7020, Test: 0.6480, Avg100: 0.6444\n",
      "Epoch: 326, Train: 0.8000, Val: 0.7020, Test: 0.6510, Avg100: 0.6446\n",
      "Epoch: 327, Train: 0.8062, Val: 0.7060, Test: 0.6610, Avg100: 0.6450\n",
      "Epoch: 328, Train: 0.8062, Val: 0.7120, Test: 0.6710, Avg100: 0.6454\n",
      "Epoch: 329, Train: 0.8062, Val: 0.7100, Test: 0.6710, Avg100: 0.6458\n",
      "Epoch: 330, Train: 0.8000, Val: 0.7160, Test: 0.6640, Avg100: 0.6462\n",
      "Epoch: 331, Train: 0.7937, Val: 0.7080, Test: 0.6550, Avg100: 0.6465\n",
      "Epoch: 332, Train: 0.7875, Val: 0.7060, Test: 0.6540, Avg100: 0.6467\n",
      "Epoch: 333, Train: 0.8000, Val: 0.7160, Test: 0.6660, Avg100: 0.6470\n",
      "Epoch: 334, Train: 0.8313, Val: 0.7520, Test: 0.7040, Avg100: 0.6478\n",
      "Epoch: 335, Train: 0.8250, Val: 0.7700, Test: 0.7220, Avg100: 0.6487\n",
      "Epoch: 336, Train: 0.8250, Val: 0.7660, Test: 0.7190, Avg100: 0.6496\n",
      "Epoch: 337, Train: 0.8188, Val: 0.7300, Test: 0.6930, Avg100: 0.6501\n",
      "Epoch: 338, Train: 0.7937, Val: 0.7100, Test: 0.6630, Avg100: 0.6504\n",
      "Epoch: 339, Train: 0.7937, Val: 0.7040, Test: 0.6570, Avg100: 0.6512\n",
      "Epoch: 340, Train: 0.7937, Val: 0.7040, Test: 0.6570, Avg100: 0.6525\n",
      "Epoch: 341, Train: 0.8062, Val: 0.7160, Test: 0.6710, Avg100: 0.6538\n",
      "Epoch: 342, Train: 0.8250, Val: 0.7240, Test: 0.6960, Avg100: 0.6551\n",
      "Epoch: 343, Train: 0.8313, Val: 0.7580, Test: 0.7110, Avg100: 0.6560\n",
      "Epoch: 344, Train: 0.8250, Val: 0.7620, Test: 0.7110, Avg100: 0.6566\n",
      "Epoch: 345, Train: 0.8250, Val: 0.7520, Test: 0.7010, Avg100: 0.6571\n",
      "Epoch: 346, Train: 0.8125, Val: 0.7240, Test: 0.6820, Avg100: 0.6572\n",
      "Epoch: 347, Train: 0.8000, Val: 0.7140, Test: 0.6670, Avg100: 0.6573\n",
      "Epoch: 348, Train: 0.8000, Val: 0.7100, Test: 0.6640, Avg100: 0.6576\n",
      "Epoch: 349, Train: 0.8062, Val: 0.7140, Test: 0.6680, Avg100: 0.6580\n",
      "Epoch: 350, Train: 0.8125, Val: 0.7160, Test: 0.6790, Avg100: 0.6585\n",
      "Epoch: 351, Train: 0.8250, Val: 0.7320, Test: 0.6980, Avg100: 0.6593\n",
      "Epoch: 352, Train: 0.8250, Val: 0.7540, Test: 0.7040, Avg100: 0.6602\n",
      "Epoch: 353, Train: 0.8125, Val: 0.7380, Test: 0.7000, Avg100: 0.6609\n",
      "Epoch: 354, Train: 0.8125, Val: 0.7300, Test: 0.6950, Avg100: 0.6615\n",
      "Epoch: 355, Train: 0.8062, Val: 0.7340, Test: 0.6980, Avg100: 0.6622\n",
      "Epoch: 356, Train: 0.8250, Val: 0.7480, Test: 0.7100, Avg100: 0.6629\n",
      "Epoch: 357, Train: 0.8313, Val: 0.7680, Test: 0.7190, Avg100: 0.6637\n",
      "Epoch: 358, Train: 0.8313, Val: 0.7720, Test: 0.7180, Avg100: 0.6646\n",
      "Epoch: 359, Train: 0.8250, Val: 0.7740, Test: 0.7180, Avg100: 0.6652\n",
      "Epoch: 360, Train: 0.8313, Val: 0.7620, Test: 0.7100, Avg100: 0.6654\n",
      "Epoch: 361, Train: 0.8000, Val: 0.7180, Test: 0.6720, Avg100: 0.6652\n",
      "Epoch: 362, Train: 0.7875, Val: 0.7040, Test: 0.6620, Avg100: 0.6648\n",
      "Epoch: 363, Train: 0.7875, Val: 0.7040, Test: 0.6620, Avg100: 0.6647\n",
      "Epoch: 364, Train: 0.8062, Val: 0.7260, Test: 0.6810, Avg100: 0.6651\n",
      "Epoch: 365, Train: 0.8250, Val: 0.7720, Test: 0.7160, Avg100: 0.6660\n",
      "Epoch: 366, Train: 0.8250, Val: 0.7700, Test: 0.7150, Avg100: 0.6668\n",
      "Epoch: 367, Train: 0.8313, Val: 0.7700, Test: 0.7140, Avg100: 0.6676\n",
      "Epoch: 368, Train: 0.8188, Val: 0.7440, Test: 0.7040, Avg100: 0.6682\n",
      "Epoch: 369, Train: 0.8125, Val: 0.7240, Test: 0.6810, Avg100: 0.6685\n",
      "Epoch: 370, Train: 0.7937, Val: 0.7120, Test: 0.6690, Avg100: 0.6688\n",
      "Epoch: 371, Train: 0.8000, Val: 0.7040, Test: 0.6660, Avg100: 0.6690\n",
      "Epoch: 372, Train: 0.8062, Val: 0.7180, Test: 0.6690, Avg100: 0.6694\n",
      "Epoch: 373, Train: 0.8188, Val: 0.7460, Test: 0.6930, Avg100: 0.6700\n",
      "Epoch: 374, Train: 0.8562, Val: 0.7800, Test: 0.7370, Avg100: 0.6710\n",
      "Epoch: 375, Train: 0.8562, Val: 0.7960, Test: 0.7360, Avg100: 0.6718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 376, Train: 0.8250, Val: 0.7780, Test: 0.7220, Avg100: 0.6725\n",
      "Epoch: 377, Train: 0.7937, Val: 0.7160, Test: 0.6750, Avg100: 0.6727\n",
      "Epoch: 378, Train: 0.7937, Val: 0.7100, Test: 0.6680, Avg100: 0.6729\n",
      "Epoch: 379, Train: 0.7937, Val: 0.7120, Test: 0.6690, Avg100: 0.6732\n",
      "Epoch: 380, Train: 0.8125, Val: 0.7320, Test: 0.6950, Avg100: 0.6738\n",
      "Epoch: 381, Train: 0.8313, Val: 0.7680, Test: 0.7130, Avg100: 0.6746\n",
      "Epoch: 382, Train: 0.8375, Val: 0.7800, Test: 0.7270, Avg100: 0.6754\n",
      "Epoch: 383, Train: 0.8438, Val: 0.7820, Test: 0.7270, Avg100: 0.6762\n",
      "Epoch: 384, Train: 0.8375, Val: 0.7700, Test: 0.7230, Avg100: 0.6768\n",
      "Epoch: 385, Train: 0.8313, Val: 0.7640, Test: 0.7160, Avg100: 0.6774\n",
      "Epoch: 386, Train: 0.8313, Val: 0.7660, Test: 0.7150, Avg100: 0.6781\n",
      "Epoch: 387, Train: 0.8250, Val: 0.7640, Test: 0.7150, Avg100: 0.6789\n",
      "Epoch: 388, Train: 0.8188, Val: 0.7480, Test: 0.7060, Avg100: 0.6795\n",
      "Epoch: 389, Train: 0.8313, Val: 0.7800, Test: 0.7200, Avg100: 0.6800\n",
      "Epoch: 390, Train: 0.8375, Val: 0.7820, Test: 0.7270, Avg100: 0.6807\n",
      "Epoch: 391, Train: 0.8375, Val: 0.7840, Test: 0.7260, Avg100: 0.6814\n",
      "Epoch: 392, Train: 0.8313, Val: 0.7700, Test: 0.7210, Avg100: 0.6822\n",
      "Epoch: 393, Train: 0.8313, Val: 0.7560, Test: 0.7120, Avg100: 0.6829\n",
      "Epoch: 394, Train: 0.8188, Val: 0.7340, Test: 0.6940, Avg100: 0.6835\n",
      "Epoch: 395, Train: 0.8062, Val: 0.7280, Test: 0.6920, Avg100: 0.6839\n",
      "Epoch: 396, Train: 0.8313, Val: 0.7440, Test: 0.7060, Avg100: 0.6844\n",
      "Epoch: 397, Train: 0.8375, Val: 0.7620, Test: 0.7130, Avg100: 0.6849\n",
      "Epoch: 398, Train: 0.8375, Val: 0.7900, Test: 0.7280, Avg100: 0.6856\n",
      "Epoch: 399, Train: 0.8375, Val: 0.7980, Test: 0.7580, Avg100: 0.6866\n",
      "Epoch: 400, Train: 0.8438, Val: 0.8040, Test: 0.7600, Avg100: 0.6879\n",
      "Epoch: 401, Train: 0.8375, Val: 0.7920, Test: 0.7370, Avg100: 0.6888\n",
      "Epoch: 402, Train: 0.8375, Val: 0.7740, Test: 0.7260, Avg100: 0.6894\n",
      "Epoch: 403, Train: 0.8313, Val: 0.7580, Test: 0.7150, Avg100: 0.6897\n",
      "Epoch: 404, Train: 0.8250, Val: 0.7440, Test: 0.7030, Avg100: 0.6898\n",
      "Epoch: 405, Train: 0.8313, Val: 0.7520, Test: 0.7100, Avg100: 0.6903\n",
      "Epoch: 406, Train: 0.8375, Val: 0.7840, Test: 0.7250, Avg100: 0.6910\n",
      "Epoch: 407, Train: 0.8438, Val: 0.7960, Test: 0.7550, Avg100: 0.6921\n",
      "Epoch: 408, Train: 0.8500, Val: 0.7980, Test: 0.7520, Avg100: 0.6931\n",
      "Epoch: 409, Train: 0.8500, Val: 0.7980, Test: 0.7490, Avg100: 0.6941\n",
      "Epoch: 410, Train: 0.8438, Val: 0.7880, Test: 0.7350, Avg100: 0.6950\n",
      "Epoch: 411, Train: 0.8125, Val: 0.7340, Test: 0.6990, Avg100: 0.6954\n",
      "Epoch: 412, Train: 0.8062, Val: 0.7180, Test: 0.6720, Avg100: 0.6954\n",
      "Epoch: 413, Train: 0.8062, Val: 0.7180, Test: 0.6720, Avg100: 0.6952\n",
      "Epoch: 414, Train: 0.8062, Val: 0.7240, Test: 0.6940, Avg100: 0.6952\n",
      "Epoch: 415, Train: 0.8375, Val: 0.7660, Test: 0.7190, Avg100: 0.6956\n",
      "Epoch: 416, Train: 0.8438, Val: 0.7940, Test: 0.7370, Avg100: 0.6964\n",
      "Epoch: 417, Train: 0.8500, Val: 0.7980, Test: 0.7550, Avg100: 0.6974\n",
      "Epoch: 418, Train: 0.8500, Val: 0.7980, Test: 0.7580, Avg100: 0.6985\n",
      "Epoch: 419, Train: 0.8438, Val: 0.7940, Test: 0.7530, Avg100: 0.6995\n",
      "Epoch: 420, Train: 0.8438, Val: 0.7840, Test: 0.7350, Avg100: 0.7002\n",
      "Epoch: 421, Train: 0.8375, Val: 0.7700, Test: 0.7290, Avg100: 0.7007\n",
      "Epoch: 422, Train: 0.8313, Val: 0.7720, Test: 0.7280, Avg100: 0.7014\n",
      "Epoch: 423, Train: 0.8375, Val: 0.7860, Test: 0.7300, Avg100: 0.7022\n",
      "Epoch: 424, Train: 0.8438, Val: 0.7920, Test: 0.7420, Avg100: 0.7031\n",
      "Epoch: 425, Train: 0.8500, Val: 0.8040, Test: 0.7480, Avg100: 0.7041\n",
      "Epoch: 426, Train: 0.8500, Val: 0.7980, Test: 0.7460, Avg100: 0.7051\n",
      "Epoch: 427, Train: 0.8438, Val: 0.7800, Test: 0.7360, Avg100: 0.7058\n",
      "Epoch: 428, Train: 0.8438, Val: 0.7700, Test: 0.7300, Avg100: 0.7064\n",
      "Epoch: 429, Train: 0.8375, Val: 0.7620, Test: 0.7270, Avg100: 0.7070\n",
      "Epoch: 430, Train: 0.8313, Val: 0.7520, Test: 0.7180, Avg100: 0.7075\n",
      "Epoch: 431, Train: 0.8375, Val: 0.7740, Test: 0.7290, Avg100: 0.7082\n",
      "Epoch: 432, Train: 0.8375, Val: 0.7860, Test: 0.7330, Avg100: 0.7090\n",
      "Epoch: 433, Train: 0.8375, Val: 0.7880, Test: 0.7460, Avg100: 0.7098\n",
      "Epoch: 434, Train: 0.8438, Val: 0.7940, Test: 0.7550, Avg100: 0.7103\n",
      "Epoch: 435, Train: 0.8562, Val: 0.7940, Test: 0.7570, Avg100: 0.7107\n",
      "Epoch: 436, Train: 0.8438, Val: 0.7920, Test: 0.7530, Avg100: 0.7110\n",
      "Epoch: 437, Train: 0.8438, Val: 0.7900, Test: 0.7480, Avg100: 0.7116\n",
      "Epoch: 438, Train: 0.8375, Val: 0.7820, Test: 0.7400, Avg100: 0.7123\n",
      "Epoch: 439, Train: 0.8375, Val: 0.7840, Test: 0.7390, Avg100: 0.7132\n",
      "Epoch: 440, Train: 0.8375, Val: 0.7920, Test: 0.7450, Avg100: 0.7140\n",
      "Epoch: 441, Train: 0.8500, Val: 0.7940, Test: 0.7510, Avg100: 0.7148\n",
      "Epoch: 442, Train: 0.8562, Val: 0.8020, Test: 0.7610, Avg100: 0.7155\n",
      "Epoch: 443, Train: 0.8500, Val: 0.7920, Test: 0.7530, Avg100: 0.7159\n",
      "Epoch: 444, Train: 0.8500, Val: 0.7780, Test: 0.7380, Avg100: 0.7162\n",
      "Epoch: 445, Train: 0.8313, Val: 0.7520, Test: 0.7280, Avg100: 0.7165\n",
      "Epoch: 446, Train: 0.8438, Val: 0.7660, Test: 0.7360, Avg100: 0.7170\n",
      "Epoch: 447, Train: 0.8562, Val: 0.7880, Test: 0.7450, Avg100: 0.7178\n",
      "Epoch: 448, Train: 0.8562, Val: 0.7940, Test: 0.7490, Avg100: 0.7186\n",
      "Epoch: 449, Train: 0.8562, Val: 0.8060, Test: 0.7600, Avg100: 0.7195\n",
      "Epoch: 450, Train: 0.8562, Val: 0.7960, Test: 0.7520, Avg100: 0.7203\n",
      "Epoch: 451, Train: 0.8375, Val: 0.7840, Test: 0.7370, Avg100: 0.7207\n",
      "Epoch: 452, Train: 0.8375, Val: 0.7840, Test: 0.7380, Avg100: 0.7210\n",
      "Epoch: 453, Train: 0.8438, Val: 0.7920, Test: 0.7540, Avg100: 0.7215\n",
      "Epoch: 454, Train: 0.8625, Val: 0.8040, Test: 0.7640, Avg100: 0.7222\n",
      "Epoch: 455, Train: 0.8625, Val: 0.8160, Test: 0.7670, Avg100: 0.7229\n",
      "Epoch: 456, Train: 0.8562, Val: 0.8060, Test: 0.7610, Avg100: 0.7234\n",
      "Epoch: 457, Train: 0.8438, Val: 0.7940, Test: 0.7560, Avg100: 0.7238\n",
      "Epoch: 458, Train: 0.8375, Val: 0.7900, Test: 0.7440, Avg100: 0.7241\n",
      "Epoch: 459, Train: 0.8438, Val: 0.7860, Test: 0.7420, Avg100: 0.7243\n",
      "Epoch: 460, Train: 0.8438, Val: 0.7840, Test: 0.7440, Avg100: 0.7246\n",
      "Epoch: 461, Train: 0.8500, Val: 0.7860, Test: 0.7430, Avg100: 0.7254\n",
      "Epoch: 462, Train: 0.8500, Val: 0.7900, Test: 0.7440, Avg100: 0.7262\n",
      "Epoch: 463, Train: 0.8500, Val: 0.7780, Test: 0.7410, Avg100: 0.7270\n",
      "Epoch: 464, Train: 0.8438, Val: 0.7760, Test: 0.7370, Avg100: 0.7275\n",
      "Epoch: 465, Train: 0.8375, Val: 0.7880, Test: 0.7430, Avg100: 0.7278\n",
      "Epoch: 466, Train: 0.8500, Val: 0.8020, Test: 0.7460, Avg100: 0.7281\n",
      "Epoch: 467, Train: 0.8562, Val: 0.7920, Test: 0.7540, Avg100: 0.7285\n",
      "Epoch: 468, Train: 0.8500, Val: 0.7940, Test: 0.7530, Avg100: 0.7290\n",
      "Epoch: 469, Train: 0.8500, Val: 0.7900, Test: 0.7470, Avg100: 0.7297\n",
      "Epoch: 470, Train: 0.8438, Val: 0.7840, Test: 0.7360, Avg100: 0.7303\n",
      "Epoch: 471, Train: 0.8375, Val: 0.7760, Test: 0.7320, Avg100: 0.7310\n",
      "Epoch: 472, Train: 0.8313, Val: 0.7720, Test: 0.7290, Avg100: 0.7316\n",
      "Epoch: 473, Train: 0.8313, Val: 0.7940, Test: 0.7400, Avg100: 0.7321\n",
      "Epoch: 474, Train: 0.8500, Val: 0.8040, Test: 0.7600, Avg100: 0.7323\n",
      "Epoch: 475, Train: 0.8625, Val: 0.8160, Test: 0.7700, Avg100: 0.7326\n",
      "Epoch: 476, Train: 0.8625, Val: 0.8080, Test: 0.7710, Avg100: 0.7331\n",
      "Epoch: 477, Train: 0.8625, Val: 0.8040, Test: 0.7680, Avg100: 0.7340\n",
      "Epoch: 478, Train: 0.8625, Val: 0.8020, Test: 0.7620, Avg100: 0.7350\n",
      "Epoch: 479, Train: 0.8438, Val: 0.7880, Test: 0.7410, Avg100: 0.7357\n",
      "Epoch: 480, Train: 0.8250, Val: 0.7540, Test: 0.7200, Avg100: 0.7360\n",
      "Epoch: 481, Train: 0.8000, Val: 0.7320, Test: 0.6990, Avg100: 0.7358\n",
      "Epoch: 482, Train: 0.8125, Val: 0.7480, Test: 0.7180, Avg100: 0.7357\n",
      "Epoch: 483, Train: 0.8313, Val: 0.7860, Test: 0.7440, Avg100: 0.7359\n",
      "Epoch: 484, Train: 0.8813, Val: 0.8160, Test: 0.7670, Avg100: 0.7363\n",
      "Epoch: 485, Train: 0.8813, Val: 0.8300, Test: 0.7820, Avg100: 0.7370\n",
      "Epoch: 486, Train: 0.8625, Val: 0.8040, Test: 0.7630, Avg100: 0.7375\n",
      "Epoch: 487, Train: 0.8250, Val: 0.7680, Test: 0.7280, Avg100: 0.7376\n",
      "Epoch: 488, Train: 0.8188, Val: 0.7460, Test: 0.7080, Avg100: 0.7376\n",
      "Epoch: 489, Train: 0.8375, Val: 0.7720, Test: 0.7310, Avg100: 0.7377\n",
      "Epoch: 490, Train: 0.8500, Val: 0.7940, Test: 0.7510, Avg100: 0.7380\n",
      "Epoch: 491, Train: 0.8625, Val: 0.8140, Test: 0.7670, Avg100: 0.7384\n",
      "Epoch: 492, Train: 0.8687, Val: 0.8240, Test: 0.7730, Avg100: 0.7389\n",
      "Epoch: 493, Train: 0.8687, Val: 0.8100, Test: 0.7710, Avg100: 0.7395\n",
      "Epoch: 494, Train: 0.8625, Val: 0.8120, Test: 0.7560, Avg100: 0.7401\n",
      "Epoch: 495, Train: 0.8313, Val: 0.7860, Test: 0.7450, Avg100: 0.7406\n",
      "Epoch: 496, Train: 0.8438, Val: 0.7860, Test: 0.7340, Avg100: 0.7409\n",
      "Epoch: 497, Train: 0.8313, Val: 0.7580, Test: 0.7230, Avg100: 0.7410\n",
      "Epoch: 498, Train: 0.8313, Val: 0.7600, Test: 0.7320, Avg100: 0.7411\n",
      "Epoch: 499, Train: 0.8500, Val: 0.7860, Test: 0.7440, Avg100: 0.7409\n",
      "Epoch: 500, Train: 0.8625, Val: 0.8140, Test: 0.7610, Avg100: 0.7409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 501, Train: 0.8875, Val: 0.8300, Test: 0.7780, Avg100: 0.7413\n",
      "Epoch: 502, Train: 0.8813, Val: 0.8360, Test: 0.7780, Avg100: 0.7419\n",
      "Epoch: 503, Train: 0.8562, Val: 0.8160, Test: 0.7620, Avg100: 0.7423\n",
      "Epoch: 504, Train: 0.8500, Val: 0.7940, Test: 0.7530, Avg100: 0.7428\n",
      "Epoch: 505, Train: 0.8500, Val: 0.7900, Test: 0.7430, Avg100: 0.7432\n",
      "Epoch: 506, Train: 0.8313, Val: 0.7640, Test: 0.7240, Avg100: 0.7432\n",
      "Epoch: 507, Train: 0.8313, Val: 0.7640, Test: 0.7250, Avg100: 0.7429\n",
      "Epoch: 508, Train: 0.8438, Val: 0.7980, Test: 0.7570, Avg100: 0.7429\n",
      "Epoch: 509, Train: 0.8562, Val: 0.8100, Test: 0.7610, Avg100: 0.7430\n",
      "Epoch: 510, Train: 0.8562, Val: 0.8180, Test: 0.7680, Avg100: 0.7434\n",
      "Epoch: 511, Train: 0.8562, Val: 0.8180, Test: 0.7610, Avg100: 0.7440\n",
      "Epoch: 512, Train: 0.8562, Val: 0.8160, Test: 0.7610, Avg100: 0.7449\n",
      "Epoch: 513, Train: 0.8438, Val: 0.8020, Test: 0.7540, Avg100: 0.7457\n",
      "Epoch: 514, Train: 0.8438, Val: 0.7880, Test: 0.7470, Avg100: 0.7462\n",
      "Epoch: 515, Train: 0.8313, Val: 0.7660, Test: 0.7320, Avg100: 0.7463\n",
      "Epoch: 516, Train: 0.8062, Val: 0.7340, Test: 0.7030, Avg100: 0.7460\n",
      "Epoch: 517, Train: 0.8375, Val: 0.7680, Test: 0.7330, Avg100: 0.7458\n",
      "Epoch: 518, Train: 0.8562, Val: 0.7960, Test: 0.7520, Avg100: 0.7457\n",
      "Epoch: 519, Train: 0.8625, Val: 0.8080, Test: 0.7600, Avg100: 0.7458\n",
      "Epoch: 520, Train: 0.8562, Val: 0.8100, Test: 0.7550, Avg100: 0.7460\n",
      "Epoch: 521, Train: 0.8500, Val: 0.8060, Test: 0.7580, Avg100: 0.7463\n",
      "Epoch: 522, Train: 0.8500, Val: 0.8140, Test: 0.7580, Avg100: 0.7466\n",
      "Epoch: 523, Train: 0.8562, Val: 0.8040, Test: 0.7630, Avg100: 0.7469\n",
      "Epoch: 524, Train: 0.8562, Val: 0.8020, Test: 0.7600, Avg100: 0.7471\n",
      "Epoch: 525, Train: 0.8562, Val: 0.7980, Test: 0.7560, Avg100: 0.7472\n",
      "Epoch: 526, Train: 0.8438, Val: 0.7880, Test: 0.7470, Avg100: 0.7472\n",
      "Epoch: 527, Train: 0.8500, Val: 0.7900, Test: 0.7470, Avg100: 0.7473\n",
      "Epoch: 528, Train: 0.8500, Val: 0.8020, Test: 0.7550, Avg100: 0.7475\n",
      "Epoch: 529, Train: 0.8438, Val: 0.8080, Test: 0.7520, Avg100: 0.7478\n",
      "Epoch: 530, Train: 0.8500, Val: 0.8120, Test: 0.7600, Avg100: 0.7482\n",
      "Epoch: 531, Train: 0.8562, Val: 0.8140, Test: 0.7610, Avg100: 0.7485\n",
      "Epoch: 532, Train: 0.8625, Val: 0.8100, Test: 0.7590, Avg100: 0.7488\n",
      "Epoch: 533, Train: 0.8562, Val: 0.7960, Test: 0.7540, Avg100: 0.7489\n",
      "Epoch: 534, Train: 0.8562, Val: 0.7900, Test: 0.7440, Avg100: 0.7488\n",
      "Epoch: 535, Train: 0.8438, Val: 0.7780, Test: 0.7400, Avg100: 0.7486\n",
      "Epoch: 536, Train: 0.8375, Val: 0.7660, Test: 0.7320, Avg100: 0.7484\n",
      "Epoch: 537, Train: 0.8438, Val: 0.7940, Test: 0.7490, Avg100: 0.7484\n",
      "Epoch: 538, Train: 0.8750, Val: 0.8080, Test: 0.7620, Avg100: 0.7486\n",
      "Epoch: 539, Train: 0.9000, Val: 0.8280, Test: 0.7790, Avg100: 0.7490\n",
      "Epoch: 540, Train: 0.8938, Val: 0.8300, Test: 0.7780, Avg100: 0.7493\n",
      "Epoch: 541, Train: 0.8687, Val: 0.8180, Test: 0.7660, Avg100: 0.7495\n",
      "Epoch: 542, Train: 0.8500, Val: 0.8000, Test: 0.7550, Avg100: 0.7494\n",
      "Epoch: 543, Train: 0.8438, Val: 0.7840, Test: 0.7410, Avg100: 0.7493\n",
      "Epoch: 544, Train: 0.8438, Val: 0.7920, Test: 0.7440, Avg100: 0.7494\n",
      "Epoch: 545, Train: 0.8562, Val: 0.8040, Test: 0.7550, Avg100: 0.7496\n",
      "Epoch: 546, Train: 0.8562, Val: 0.8140, Test: 0.7670, Avg100: 0.7500\n",
      "Epoch: 547, Train: 0.8750, Val: 0.8280, Test: 0.7800, Avg100: 0.7503\n",
      "Epoch: 548, Train: 0.8750, Val: 0.8220, Test: 0.7750, Avg100: 0.7506\n",
      "Epoch: 549, Train: 0.8625, Val: 0.8040, Test: 0.7600, Avg100: 0.7506\n",
      "Epoch: 550, Train: 0.8313, Val: 0.7780, Test: 0.7300, Avg100: 0.7503\n",
      "Epoch: 551, Train: 0.8062, Val: 0.7380, Test: 0.7080, Avg100: 0.7501\n",
      "Epoch: 552, Train: 0.8062, Val: 0.7540, Test: 0.7130, Avg100: 0.7498\n",
      "Epoch: 553, Train: 0.8313, Val: 0.7840, Test: 0.7380, Avg100: 0.7496\n",
      "Epoch: 554, Train: 0.8500, Val: 0.8060, Test: 0.7600, Avg100: 0.7496\n",
      "Epoch: 555, Train: 0.8750, Val: 0.8260, Test: 0.7840, Avg100: 0.7498\n",
      "Epoch: 556, Train: 0.9000, Val: 0.8440, Test: 0.7960, Avg100: 0.7501\n",
      "Epoch: 557, Train: 0.9000, Val: 0.8440, Test: 0.7970, Avg100: 0.7505\n",
      "Epoch: 558, Train: 0.8750, Val: 0.8240, Test: 0.7800, Avg100: 0.7509\n",
      "Epoch: 559, Train: 0.8562, Val: 0.8020, Test: 0.7580, Avg100: 0.7511\n",
      "Epoch: 560, Train: 0.8375, Val: 0.7780, Test: 0.7360, Avg100: 0.7510\n",
      "Epoch: 561, Train: 0.8125, Val: 0.7340, Test: 0.7020, Avg100: 0.7506\n",
      "Epoch: 562, Train: 0.8062, Val: 0.7200, Test: 0.6920, Avg100: 0.7500\n",
      "Epoch: 563, Train: 0.8062, Val: 0.7260, Test: 0.6980, Avg100: 0.7496\n",
      "Epoch: 564, Train: 0.8375, Val: 0.7720, Test: 0.7310, Avg100: 0.7496\n",
      "Epoch: 565, Train: 0.8562, Val: 0.7960, Test: 0.7520, Avg100: 0.7496\n",
      "Epoch: 566, Train: 0.8687, Val: 0.8160, Test: 0.7630, Avg100: 0.7498\n",
      "Epoch: 567, Train: 0.8750, Val: 0.8280, Test: 0.7800, Avg100: 0.7501\n",
      "Epoch: 568, Train: 0.8625, Val: 0.8180, Test: 0.7650, Avg100: 0.7502\n",
      "Epoch: 569, Train: 0.8562, Val: 0.8020, Test: 0.7510, Avg100: 0.7502\n",
      "Epoch: 570, Train: 0.8375, Val: 0.7960, Test: 0.7460, Avg100: 0.7503\n",
      "Epoch: 571, Train: 0.8250, Val: 0.7860, Test: 0.7380, Avg100: 0.7504\n",
      "Epoch: 572, Train: 0.8250, Val: 0.7840, Test: 0.7340, Avg100: 0.7504\n",
      "Epoch: 573, Train: 0.8313, Val: 0.7860, Test: 0.7360, Avg100: 0.7504\n",
      "Epoch: 574, Train: 0.8562, Val: 0.7900, Test: 0.7460, Avg100: 0.7503\n",
      "Epoch: 575, Train: 0.8625, Val: 0.8060, Test: 0.7580, Avg100: 0.7501\n",
      "Epoch: 576, Train: 0.8625, Val: 0.8020, Test: 0.7600, Avg100: 0.7500\n",
      "Epoch: 577, Train: 0.8687, Val: 0.8060, Test: 0.7570, Avg100: 0.7499\n",
      "Epoch: 578, Train: 0.8687, Val: 0.8020, Test: 0.7530, Avg100: 0.7498\n",
      "Epoch: 579, Train: 0.8625, Val: 0.7940, Test: 0.7460, Avg100: 0.7499\n",
      "Epoch: 580, Train: 0.8625, Val: 0.8000, Test: 0.7500, Avg100: 0.7502\n",
      "Epoch: 581, Train: 0.8625, Val: 0.8060, Test: 0.7600, Avg100: 0.7508\n",
      "Epoch: 582, Train: 0.8562, Val: 0.7980, Test: 0.7540, Avg100: 0.7512\n",
      "Epoch: 583, Train: 0.8500, Val: 0.7920, Test: 0.7500, Avg100: 0.7512\n",
      "Epoch: 584, Train: 0.8500, Val: 0.7860, Test: 0.7440, Avg100: 0.7510\n",
      "Epoch: 585, Train: 0.8313, Val: 0.7780, Test: 0.7330, Avg100: 0.7505\n",
      "Epoch: 586, Train: 0.8313, Val: 0.7820, Test: 0.7320, Avg100: 0.7502\n",
      "Epoch: 587, Train: 0.8313, Val: 0.7820, Test: 0.7330, Avg100: 0.7502\n",
      "Epoch: 588, Train: 0.8687, Val: 0.8040, Test: 0.7570, Avg100: 0.7507\n",
      "Epoch: 589, Train: 0.8813, Val: 0.8240, Test: 0.7820, Avg100: 0.7512\n",
      "Epoch: 590, Train: 0.8938, Val: 0.8400, Test: 0.7910, Avg100: 0.7516\n",
      "Epoch: 591, Train: 0.8813, Val: 0.8300, Test: 0.7810, Avg100: 0.7518\n",
      "Epoch: 592, Train: 0.8687, Val: 0.8100, Test: 0.7650, Avg100: 0.7517\n",
      "Epoch: 593, Train: 0.8438, Val: 0.7940, Test: 0.7520, Avg100: 0.7515\n",
      "Epoch: 594, Train: 0.8313, Val: 0.7920, Test: 0.7430, Avg100: 0.7514\n",
      "Epoch: 595, Train: 0.8250, Val: 0.7880, Test: 0.7360, Avg100: 0.7513\n",
      "Epoch: 596, Train: 0.8438, Val: 0.7820, Test: 0.7380, Avg100: 0.7513\n",
      "Epoch: 597, Train: 0.8500, Val: 0.7900, Test: 0.7510, Avg100: 0.7516\n",
      "Epoch: 598, Train: 0.8562, Val: 0.8000, Test: 0.7600, Avg100: 0.7519\n",
      "Epoch: 599, Train: 0.8625, Val: 0.8060, Test: 0.7600, Avg100: 0.7520\n",
      "Epoch: 600, Train: 0.8625, Val: 0.8020, Test: 0.7590, Avg100: 0.7520\n",
      "Epoch: 601, Train: 0.8562, Val: 0.8060, Test: 0.7630, Avg100: 0.7519\n",
      "Epoch: 602, Train: 0.8625, Val: 0.8020, Test: 0.7550, Avg100: 0.7516\n",
      "Epoch: 603, Train: 0.8438, Val: 0.7980, Test: 0.7470, Avg100: 0.7515\n",
      "Epoch: 604, Train: 0.8438, Val: 0.7920, Test: 0.7490, Avg100: 0.7515\n",
      "Epoch: 605, Train: 0.8438, Val: 0.7960, Test: 0.7510, Avg100: 0.7515\n",
      "Epoch: 606, Train: 0.8438, Val: 0.7940, Test: 0.7520, Avg100: 0.7518\n",
      "Epoch: 607, Train: 0.8313, Val: 0.7920, Test: 0.7460, Avg100: 0.7520\n",
      "Epoch: 608, Train: 0.8438, Val: 0.7860, Test: 0.7440, Avg100: 0.7519\n",
      "Epoch: 609, Train: 0.8438, Val: 0.7920, Test: 0.7490, Avg100: 0.7518\n",
      "Epoch: 610, Train: 0.8438, Val: 0.7880, Test: 0.7420, Avg100: 0.7515\n",
      "Epoch: 611, Train: 0.8500, Val: 0.7800, Test: 0.7460, Avg100: 0.7514\n",
      "Epoch: 612, Train: 0.8500, Val: 0.7640, Test: 0.7290, Avg100: 0.7510\n",
      "Epoch: 613, Train: 0.8500, Val: 0.7720, Test: 0.7230, Avg100: 0.7507\n",
      "Epoch: 614, Train: 0.8438, Val: 0.7600, Test: 0.7270, Avg100: 0.7505\n",
      "Epoch: 615, Train: 0.8438, Val: 0.7640, Test: 0.7230, Avg100: 0.7504\n",
      "Epoch: 616, Train: 0.8438, Val: 0.7800, Test: 0.7350, Avg100: 0.7508\n",
      "Epoch: 617, Train: 0.8313, Val: 0.7800, Test: 0.7320, Avg100: 0.7508\n",
      "Epoch: 618, Train: 0.8438, Val: 0.7900, Test: 0.7470, Avg100: 0.7507\n",
      "Epoch: 619, Train: 0.8500, Val: 0.8020, Test: 0.7540, Avg100: 0.7506\n",
      "Epoch: 620, Train: 0.8562, Val: 0.7980, Test: 0.7590, Avg100: 0.7507\n",
      "Epoch: 621, Train: 0.8562, Val: 0.8060, Test: 0.7630, Avg100: 0.7507\n",
      "Epoch: 622, Train: 0.8625, Val: 0.8120, Test: 0.7710, Avg100: 0.7509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 623, Train: 0.8625, Val: 0.8120, Test: 0.7710, Avg100: 0.7509\n",
      "Epoch: 624, Train: 0.8562, Val: 0.8140, Test: 0.7670, Avg100: 0.7510\n",
      "Epoch: 625, Train: 0.8562, Val: 0.8080, Test: 0.7590, Avg100: 0.7510\n",
      "Epoch: 626, Train: 0.8438, Val: 0.7960, Test: 0.7510, Avg100: 0.7511\n",
      "Epoch: 627, Train: 0.8375, Val: 0.7960, Test: 0.7490, Avg100: 0.7511\n",
      "Epoch: 628, Train: 0.8500, Val: 0.8020, Test: 0.7520, Avg100: 0.7511\n",
      "Epoch: 629, Train: 0.8625, Val: 0.8100, Test: 0.7620, Avg100: 0.7512\n",
      "Epoch: 630, Train: 0.8625, Val: 0.8060, Test: 0.7600, Avg100: 0.7512\n",
      "Epoch: 631, Train: 0.8500, Val: 0.7980, Test: 0.7570, Avg100: 0.7511\n",
      "Epoch: 632, Train: 0.8500, Val: 0.7940, Test: 0.7490, Avg100: 0.7510\n",
      "Epoch: 633, Train: 0.8500, Val: 0.7840, Test: 0.7480, Avg100: 0.7510\n",
      "Epoch: 634, Train: 0.8562, Val: 0.7780, Test: 0.7430, Avg100: 0.7510\n",
      "Epoch: 635, Train: 0.8500, Val: 0.7840, Test: 0.7420, Avg100: 0.7510\n",
      "Epoch: 636, Train: 0.8562, Val: 0.7860, Test: 0.7490, Avg100: 0.7512\n",
      "Epoch: 637, Train: 0.8625, Val: 0.7980, Test: 0.7630, Avg100: 0.7513\n",
      "Epoch: 638, Train: 0.8938, Val: 0.8280, Test: 0.7900, Avg100: 0.7516\n",
      "Epoch: 639, Train: 0.9000, Val: 0.8360, Test: 0.7920, Avg100: 0.7517\n",
      "Epoch: 640, Train: 0.8813, Val: 0.8240, Test: 0.7740, Avg100: 0.7517\n",
      "Epoch: 641, Train: 0.8562, Val: 0.8040, Test: 0.7560, Avg100: 0.7516\n",
      "Epoch: 642, Train: 0.8438, Val: 0.7880, Test: 0.7380, Avg100: 0.7514\n",
      "Epoch: 643, Train: 0.8313, Val: 0.7820, Test: 0.7340, Avg100: 0.7513\n",
      "Epoch: 644, Train: 0.8500, Val: 0.7920, Test: 0.7480, Avg100: 0.7514\n",
      "Epoch: 645, Train: 0.8562, Val: 0.8020, Test: 0.7580, Avg100: 0.7514\n",
      "Epoch: 646, Train: 0.8687, Val: 0.8080, Test: 0.7670, Avg100: 0.7514\n",
      "Epoch: 647, Train: 0.8750, Val: 0.8220, Test: 0.7740, Avg100: 0.7513\n",
      "Epoch: 648, Train: 0.8625, Val: 0.8120, Test: 0.7650, Avg100: 0.7512\n",
      "Epoch: 649, Train: 0.8562, Val: 0.8060, Test: 0.7500, Avg100: 0.7511\n",
      "Epoch: 650, Train: 0.8438, Val: 0.7540, Test: 0.7190, Avg100: 0.7510\n",
      "Epoch: 651, Train: 0.8125, Val: 0.7360, Test: 0.6880, Avg100: 0.7508\n",
      "Epoch: 652, Train: 0.8188, Val: 0.7400, Test: 0.6930, Avg100: 0.7506\n",
      "Epoch: 653, Train: 0.8375, Val: 0.7620, Test: 0.7210, Avg100: 0.7505\n",
      "Epoch: 654, Train: 0.8562, Val: 0.7920, Test: 0.7500, Avg100: 0.7504\n",
      "Epoch: 655, Train: 0.8750, Val: 0.8300, Test: 0.7820, Avg100: 0.7503\n",
      "Early Stopping at epoch 656, Val: 0.8440, Test: 0.8010\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Amazon\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv\n",
    "import numpy as np\n",
    "\n",
    "dataset = \"Photo\"\n",
    "path = osp.join('data', dataset)\n",
    "dataset = Amazon(path, dataset, transform=T.NormalizeFeatures())\n",
    "dataset.shuffle()\n",
    "data = dataset[0]\n",
    "\n",
    "data = dataset[0]\n",
    "TRAIN_END = 20 * dataset.num_classes\n",
    "VAL_END = TRAIN_END + 500\n",
    "TEST_END = VAL_END + 1000\n",
    "data = dataset[0]\n",
    "data.train_mask = torch.zeros([data.num_nodes,], dtype=torch.bool)\n",
    "data.train_mask[:TRAIN_END] = True\n",
    "data.val_mask = torch.zeros([data.num_nodes,], dtype=torch.bool)\n",
    "data.val_mask[TRAIN_END:VAL_END] = True\n",
    "data.test_mask = torch.zeros([data.num_nodes,], dtype=torch.bool)\n",
    "data.test_mask[VAL_END:TEST_END] = True\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = GATConv(in_channels, 8, heads=8, dropout=0.6)\n",
    "        # On the Pubmed dataset, use heads=8 in conv2.\n",
    "        self.conv2 = GATConv(8 * 8, out_channels, heads=1, concat=False,\n",
    "                             dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    out, accs = model(data.x, data.edge_index), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        acc = float((out[mask].argmax(-1) == data.y[mask]).sum() / mask.sum())\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "print()\n",
    "avg = np.empty([1,])\n",
    "max_val = 0\n",
    "early_stop_cnt = 0\n",
    "for epoch in range(1, 1001):\n",
    "    train(data)\n",
    "    train_acc, val_acc, test_acc = test(data)\n",
    "\n",
    "    if val_acc > max_val:\n",
    "        early_stop_cnt = 0\n",
    "        max_val = val_acc\n",
    "    else:\n",
    "        early_stop_cnt += 1\n",
    "        if (early_stop_cnt == 100):\n",
    "            print(f'Early Stopping at epoch {epoch:03d}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "            break\n",
    "\n",
    "    avg = np.append(avg, test_acc)\n",
    "    last_hun = np.average(avg[len(avg)-100:])\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, '\n",
    "          f'Test: {test_acc:.4f}, Avg100: {last_hun:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
